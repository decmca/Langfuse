# Model Configurations

embedders:
  baseline:
    name: "sentence-transformers/all-MiniLM-L6-v2"
    dimension: 384
    max_seq_length: 256
    
  improved:
    name: "BAAI/bge-large-en-v1.5"
    dimension: 1024
    max_seq_length: 512

generators:
  qwen:
    name: "Qwen/Qwen2.5-7B-Instruct"
    context_length: 32768
    load_in_8bit: false
    load_in_4bit: false
    torch_dtype: "bfloat16"
    
  deepseek:
    name: "deepseek-ai/deepseek-llm-7b-chat"
    context_length: 4096
    load_in_8bit: false
    load_in_4bit: false
    torch_dtype: "bfloat16"
    
  llama:
    name: "meta-llama/Llama-3.2-8B-Instruct"
    context_length: 8192
    load_in_8bit: false
    load_in_4bit: false
    torch_dtype: "bfloat16"

# Retrieval settings
retrieval:
  top_k: 5
  similarity_metric: "cosine"
  use_reranking: false
  reranker_model: null

# Generation settings
generation:
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  do_sample: true
  num_return_sequences: 1
